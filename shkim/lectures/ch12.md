# 에필로그 (아파치 스파크 3.0)

## 스파크 코어와 스파크 SQL
* 쿼리를 빠르게 처리하는 방법
  * 동적 파티션 정리를 써서 데이터를 적게 읽는 것.
  * 쿼리 실행 계획을 조정하고 최적화

<br>

### 동적 파티션 정리 (Dynamic Partition Pruning)
* 쿼리에 필요 없는 데이터는 처리 없이 넘어감.
* Fact 테이블과 Dimension 테이블을 조인할 때 
* 디멘션 테이블로부터 필터링한 결과를 가져다가 데이터를 읽어 들이는 범위를 제한하기 위해 팩트 테이블에 스캔 일부 결과로 집어넣는 것.
* 디멘션 테이블이 팩트 테이브보다 작고, 조인을 수행하는 경우 브로드캐스트 조인을 시도함.
* 잘 이해 안됨..
* 스파크 3.0에서부터는 스타 스키마 쿼리에 대해 뛰어난 성능을 보임.

<br>

### 적응형 쿼리 실행
* 실행 중에 물리 실행 계획을 적응
  * 셔플 파티션 개수를 줄여 리듀서의 개수를 줄임
  * 쿼리의 물리 실행 계획을 최적화
  * 조인하는 동안 데이터 스큐를 처리
  * `spark.sql.adaptive.enabled` 속성을 true로 설정

<br>

**AQE 프레임워크**
* 한 쿼리에서의 연산들은 파이프  라이닝 배치되고 병렬 처리로  실행되지만 셔플이나 브로드캐스트 교환 같은 경우 파이프라인을 끊게됨.
* AQE 프레임워크가 반복적으로 수행되는 개념
  * 각 스테이지에서 모든 스캔 연산 같은 리프 노드들이 실행됨
  * 한번 구체화 포인트가 끝나면 완료로 표시. 통계 정보들은 논리 계획에 사용
  * 통계 정보 (읽은 파티션 개수, 데이터 바이트수 등)에 기반해서 프레임워크는 카탈리스트 옵티마이저를 실행하여  다음 내용들을 수행할 수 있는지 파악
    * 파티션의 개수를 줄여 리듀서의 개수를 줄인다.
    * 소트 머지 조인을 테이블에서 읽어 들이는 데이터 크기에 기반해 브로드캐스트 조인으로 변경
    * 스큐 조인을 해결하고자 시도
    * 새로운 최적화 논리 계획을  작성하고 새로운 최적화 물리화 계획을 만듬.

<br>

### SQL 조인 힌트
**셔플 소트 머지 조인(SMJ)**
* SELECT 문장에서 /*+ .. */ 주석 안에 하나이상의 힌트를 줄 수 있음
```sql
SELECT /*+ MERGE(a, b) */ id FROM a JOIN b ON a.key = b.key
# customers와 orders를 조인할 때 SortMergeJoin을 쓰도록 지시
SELECT /*+ MERGE(customers, orders) */ * FROM customers, orders WHERE order.custId = customers.custId
```

<br>

**브로드캐스트 해시 조인(BHJ)**
* 브로드캐스트 조인을 하도록 힌트 제공
```sql
SELECT /*+ BROADCAST(a) */ id FROM a JOIN b ON a.key = b.key
# customers와 orders를 조인할 때 BroadcastHashJoin을 쓰도록 지시
SELECT /*+ BROADCAST(customers) */ * FROM customers, orders WHERE order.custId = customers.custId
```

<br>

**셔플 해시 조인(SHJ)**
```sql
SELECT /*+ SHUFFLE_HASH(a, b) */ id FROM a JOIN b ON a.key = b.key
SELECT /*+ SHUFFLE_HASH(customers, orders) */ * FROM customers, orders WHERE order.custId = customers.custId
```

<br>

**셔플-복제 다중 루프 조인(SNLJ)**
```sql
SELECT /*+ SHUFFLE_REPLICATE_NL(a, b) */ id FROM a JOIN b
```

<br>

### 카탈로그 플러그인 API와 데이터 소스 V2
* 데이터 소스 V2 API는 다음 기능을 제공
  * 카탈로그와 테이블 관리를 위한 외부 데이터 소스 플러그인 사용이 가능함
  * ORC, 파케이등 외에도 추가적인 데이터 소스들에 대해 지원을 한다.
  * 싱크와 소스들에 대한 데이터 소스들의 스트리밍과 배치 처리를 위해 공통적인 API를 지원함.

* 플러그인 가능한 카탈로그를 쓰려면 다음 설정을 해야함.
```editorconfig
spark.sql.catalog.ndb_catalog com.ndb.ConnectorImpl
spark.sql.catalog.ndb_catalog.option1 value1
spark.sql.catalog.ndb_catalog.option2 value2     
```

* 아래와 같이 지정하면 스파크나 SQL 사용자는 DataFrameReader와 DataFrameWriter API 사용 가능.
```sql
SHOW TABLES ndb_catalog;
CREATE TABLE ndb_catalog.table_1;
SELECT * FROM ndb_catalog.table_1;
ALTER TABLE ndb_catalog.table_1
```

```scala
df.writeTo("ndb_catalog.table_1")
val dfNBD = spark.read.table("ndb_catalog.table_1")
                      .option("option1", "value1")
                      .option("option2", "value2")
```

<br>

### 가속 감지 스케줄러
* AI와 빅데이터를 동시에 다루기 위한 커뮤니티 주도 프로젝트 하이드로젠
  * 배리어 실행 모드 구현 : 스파크 2.4.0에 도입
  * 가속 감지 스케줄링
  * 데이터 교환 최적화
