{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, IntegerType\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Streaming from Kafka\") \\\n",
    "    .config(\"spark.streaming.stopGracefullyOnShutdown\", True) \\\n",
    "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0') \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", 4) \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KAFKA_BOOTSTRAP_SERVERS= \"kafka1:19091,kafka2:19092,kafka3:19093\"\n",
    "schema = StructType([\n",
    "    StructField(\"eventId\", StringType()),\n",
    "    StructField(\"eventOffset\", StringType()),\n",
    "    StructField(\"eventPublisher\", StringType()),\n",
    "    StructField(\"customerId\", StringType()),\n",
    "    StructField(\"data\", StructType([\n",
    "        StructField(\"devices\", ArrayType(StructType([\n",
    "            StructField(\"deviceId\", StringType()),\n",
    "            StructField(\"temperature\", IntegerType()),\n",
    "            StructField(\"measure\", StringType()),\n",
    "            StructField(\"status\", StringType()),\n",
    "        ])))\n",
    "    ])),\n",
    "    StructField(\"eventTime\", StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark \\\n",
    "    .read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVERS) \\\n",
    "    .option(\"subscribe\", \"devices\") \\\n",
    "    .load() \\\n",
    "    .select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"parsed_value\")) \\\n",
    "    .select(col(\"parsed_value.*\"))\n",
    "df.show(10, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVERS) \\\n",
    "    .option(\"subscribe\", \"devices\") \\\n",
    "    .load() \\\n",
    "    .select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"parsed_value\")) \\\n",
    "    .select(col(\"parsed_value.*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = lines \\\n",
    "    .writeStream \\\n",
    "    .trigger(processingTime='5 seconds')\n",
    "    .outputMode('Append') \\\n",
    "    .format('console') \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
